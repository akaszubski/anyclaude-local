{
  "backend": "vllm-mlx",
  "debug": {
    "level": 0,
    "enableTraces": false,
    "enableStreamLogging": false
  },
  "backends": {
    "lmstudio": {
      "enabled": true,
      "baseUrl": "http://localhost:1234/v1",
      "apiKey": "lm-studio",
      "model": "current-model",
      "compatibility": "legacy",
      "description": "LMStudio - uses the model currently loaded in LMStudio. Just provide baseUrl and apiKey."
    },
    "vllm-mlx": {
      "enabled": true,
      "port": 8081,
      "baseUrl": "http://localhost:8081/v1",
      "apiKey": "vllm-mlx",
      "model": "/Users/akaszubski/ai-tools/lmstudio/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit",
      "serverScript": "scripts/vllm-mlx-server-lite.py",
      "description": "vLLM-MLX - high-performance inference server with Qwen3 Coder 30B"
    }
  }
}
