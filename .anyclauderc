# AnyClaude Configuration File
# Place this file in your project root or home directory (~/.anyclauderc)
# Configuration is read in order of precedence:
#   1. Environment variables (highest priority)
#   2. ~/.anyclauderc (home directory)
#   3. .anyclauderc (project directory)
#   4. Defaults (lowest priority)

# ============================================================================
# Mode Selection
# ============================================================================
# Options: mlx-omni, mlx-lm
#
# mlx-omni: For 30x faster follow-ups via KV cache (requires pre-loaded model)
#   - ARCHITECTURAL LIMITATION: Does NOT accept --model parameter
#   - Model must be pre-configured in MLX-Omni environment before running
#   - Native Anthropic API format
#   - Performance: 1st question ~30-40s, follow-ups <1s (via KV cache)
#   - For local model files, use mlx-lm instead (has config file control)
#   - Example: ./anyclaude mlx-omni (requires model pre-loaded)
#
# mlx-lm: Use when you have local model files or want HuggingFace model IDs
#   - Supports local paths AND HuggingFace IDs
#   - OpenAI-compatible API format
#   - Performance: Consistent ~25-40s per request (no KV cache)
#   - Example: ./anyclaude mlx-lm
#
# Use mlx-omni-local for KV cache with local models
# This script pre-loads your model, then launches MLX-Omni for fast follow-ups
# Alternative: mlx-lm (config file control, no KV cache)
ANYCLAUDE_MODE=mlx-omni-local

# ============================================================================
# Model Configuration
# ============================================================================
# MLX-Omni: Config file controls startup model
#   Supports both HuggingFace IDs and local model paths
#   Examples:
#     mlx-community/Qwen2.5-0.5B-Instruct-4bit  (HuggingFace ID)
#     mlx-community/Qwen2.5-1.5B-Instruct-4bit  (HuggingFace ID)
#     mlx-community/Qwen2.5-3B-Instruct-4bit    (HuggingFace ID)
#     /path/to/local/model                      (Local model file path)
#
# MLX-LM: Config file controls startup model
#   Supports both HuggingFace IDs and local model paths
#   Examples:
#     /Users/akaszubski/ai-tools/lmstudio/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit
#     mlx-community/Qwen2.5-1.5B-Instruct-4bit

# Set the model for both MLX-LM and MLX-Omni
MLX_MODEL=/Users/akaszubski/ai-tools/lmstudio/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit
MLX_OMNI_MODEL=/Users/akaszubski/ai-tools/lmstudio/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit

# ============================================================================
# Debug Logging
# ============================================================================
# Options: 0 (none), 1 (basic), 2 (verbose), 3 (trace with tool calls)
ANYCLAUDE_DEBUG=0

# ============================================================================
# Port Configuration (optional)
# ============================================================================
# MLX-Omni port (default: 8080)
# MLX_OMNI_PORT=8080

# MLX-LM port (default: 8081)
# MLX_LM_PORT=8081

# ============================================================================
# Virtual Environment Path (MLX-LM only)
# ============================================================================
# Path to your Python virtual environment (default: ~/.venv-mlx)
# VENV_PATH=/path/to/venv

# ============================================================================
# Development Options
# ============================================================================
# Run proxy without spawning Claude Code (for testing)
# PROXY_ONLY=true
