# AnyClaude Configuration File
# Place this file in your project root or home directory (~/.anyclauderc)
# Configuration is read in order of precedence:
#   1. Environment variables (highest priority)
#   2. ~/.anyclauderc (home directory)
#   3. .anyclauderc (project directory)
#   4. Defaults (lowest priority)

# ============================================================================
# Mode Selection
# ============================================================================
# Options: mlx-omni, mlx-lm
#
# mlx-omni: RECOMMENDED for HuggingFace models with 30x faster follow-ups via KV cache
#   - Only works with HuggingFace model IDs (mlx-community/...)
#   - Native Anthropic API format
#   - Performance: 1st question ~30-40s, follow-ups <1s (via KV cache)
#   - Example: ./anyclaude mlx-omni
#
# mlx-lm: Use when you have local model files or want HuggingFace model IDs
#   - Supports local paths AND HuggingFace IDs
#   - OpenAI-compatible API format
#   - Performance: Consistent ~25-40s per request (no KV cache)
#   - Example: ./anyclaude mlx-lm
#
# Set to mlx-lm by default to use your local Qwen3-Coder model
# Uncomment the next line to use mlx-omni with KV cache:
# ANYCLAUDE_MODE=mlx-omni
ANYCLAUDE_MODE=mlx-lm

# ============================================================================
# Model Configuration
# ============================================================================
# MLX-Omni: Config file controls startup model
#   Supports both HuggingFace IDs and local model paths
#   Examples:
#     mlx-community/Qwen2.5-0.5B-Instruct-4bit  (HuggingFace ID)
#     mlx-community/Qwen2.5-1.5B-Instruct-4bit  (HuggingFace ID)
#     mlx-community/Qwen2.5-3B-Instruct-4bit    (HuggingFace ID)
#     /path/to/local/model                      (Local model file path)
#
# MLX-LM: Config file controls startup model
#   Supports both HuggingFace IDs and local model paths
#   Examples:
#     /Users/akaszubski/ai-tools/lmstudio/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit
#     mlx-community/Qwen2.5-1.5B-Instruct-4bit

MLX_MODEL=/Users/akaszubski/ai-tools/lmstudio/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit

# ============================================================================
# Debug Logging
# ============================================================================
# Options: 0 (none), 1 (basic), 2 (verbose), 3 (trace with tool calls)
ANYCLAUDE_DEBUG=0

# ============================================================================
# Port Configuration (optional)
# ============================================================================
# MLX-Omni port (default: 8080)
# MLX_OMNI_PORT=8080

# MLX-LM port (default: 8081)
# MLX_LM_PORT=8081

# ============================================================================
# Virtual Environment Path (MLX-LM only)
# ============================================================================
# Path to your Python virtual environment (default: ~/.venv-mlx)
# VENV_PATH=/path/to/venv

# ============================================================================
# Development Options
# ============================================================================
# Run proxy without spawning Claude Code (for testing)
# PROXY_ONLY=true
