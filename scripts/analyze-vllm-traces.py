#!/usr/bin/env python3
"""
Analyze MLX trace files to benchmark performance and cache effectiveness.

This script reads the trace files generated by Claude Code and extracts:
- Request/response timing (latency)
- Cache hit/miss detection (identical requests)
- Tool calling effectiveness
- Error analysis
"""

import json
import glob
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import hashlib

class TraceAnalyzer:
    def __init__(self, trace_dir=None):
        if trace_dir is None:
            trace_dir = Path.home() / ".anyclaude" / "traces" / "mlx"

        self.trace_dir = Path(trace_dir)
        self.traces = []
        self.load_traces()

    def load_traces(self):
        """Load all trace files"""
        if not self.trace_dir.exists():
            print(f"Trace directory not found: {self.trace_dir}")
            return

        trace_files = sorted(self.trace_dir.glob("*.json"))
        print(f"ðŸ“ Found {len(trace_files)} trace files in {self.trace_dir}\n")

        for trace_file in trace_files:
            try:
                with open(trace_file) as f:
                    trace = json.load(f)
                self.traces.append(trace)
            except Exception as e:
                print(f"âš ï¸  Error reading {trace_file.name}: {e}")

    def get_request_hash(self, trace):
        """Generate a hash of the request to detect duplicates (cache hits)"""
        try:
            req_body = trace.get("request", {}).get("body", {})

            # Key parts for cache detection
            messages = json.dumps(req_body.get("messages", []), sort_keys=True, default=str)
            tools = json.dumps(req_body.get("tools", []), sort_keys=True, default=str)
            model = req_body.get("model", "unknown")

            combined = f"{messages}|{tools}|{model}"
            return hashlib.md5(combined.encode()).hexdigest()[:8]
        except:
            return None

    def get_timing_info(self, trace):
        """Extract timing information from trace"""
        timestamp = trace.get("timestamp", "")

        try:
            # Parse ISO timestamp
            dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
            return {
                "timestamp": timestamp,
                "datetime": dt,
                "unix_ms": int(dt.timestamp() * 1000)
            }
        except:
            return {"timestamp": timestamp, "datetime": None, "unix_ms": 0}

    def analyze_cache_effectiveness(self):
        """Detect cache hits by finding identical requests"""
        print("=" * 100)
        print("CACHE EFFECTIVENESS ANALYSIS")
        print("=" * 100)
        print()

        # Group requests by hash
        request_groups = defaultdict(list)

        for i, trace in enumerate(self.traces):
            req_hash = self.get_request_hash(trace)
            timing = self.get_timing_info(trace)

            request_groups[req_hash].append({
                "index": i,
                "timestamp": timing["timestamp"],
                "trace": trace
            })

        # Analyze groups with multiple requests (potential cache hits)
        cache_hits = []
        cache_misses = []

        for req_hash, group in request_groups.items():
            if len(group) == 1:
                cache_misses.append(group[0])
            else:
                # This request appears multiple times - likely cache hits
                cache_hits.extend(group[1:])  # First is miss, rest are hits

                print(f"ðŸŽ¯ Request appears {len(group)} times:")
                print(f"   Hash: {req_hash}")

                # Show timing between requests
                for j, req in enumerate(group):
                    req_body = req["trace"].get("request", {}).get("body", {})
                    msg_count = len(req_body.get("messages", []))
                    tool_count = len(req_body.get("tools", []))

                    if j == 0:
                        print(f"     [0] FIRST:  {req['timestamp']} (messages: {msg_count}, tools: {tool_count})")
                    else:
                        # Calculate time since first request
                        prev_timestamp = group[0]['timestamp']
                        print(f"     [{j}] REPEAT: {req['timestamp']} (messages: {msg_count}, tools: {tool_count})")
                        print(f"        â†³ This is a cache hit candidate (duplicate request)")

                print()

        hit_rate = len(cache_hits) / len(self.traces) * 100 if self.traces else 0

        print(f"\nðŸ“Š CACHE SUMMARY:")
        print(f"   Total requests: {len(self.traces)}")
        print(f"   Unique requests: {len(request_groups)}")
        print(f"   Potential cache hits: {len(cache_hits)}")
        print(f"   Cache hit rate: {hit_rate:.1f}%")
        print()

    def analyze_request_patterns(self):
        """Analyze patterns in requests"""
        print("=" * 100)
        print("REQUEST PATTERNS")
        print("=" * 100)
        print()

        message_counts = []
        tool_counts = []
        tools_used = defaultdict(int)

        for trace in self.traces:
            req_body = trace.get("request", {}).get("body", {})
            messages = req_body.get("messages", [])
            tools = req_body.get("tools", [])

            message_counts.append(len(messages))
            tool_counts.append(len(tools))

            # Track tool usage
            for tool in tools:
                tool_name = tool.get("name", "unknown")
                tools_used[tool_name] += 1

        if message_counts:
            print(f"ðŸ“¨ MESSAGE COUNTS:")
            print(f"   Min: {min(message_counts)}, Max: {max(message_counts)}, Avg: {sum(message_counts)/len(message_counts):.1f}")
            print()

        if tool_counts:
            print(f"ðŸ”§ TOOL USAGE:")
            print(f"   Requests with tools: {sum(1 for c in tool_counts if c > 0)}/{len(tool_counts)}")
            print(f"   Tools per request: Min: {min(tool_counts)}, Max: {max(tool_counts)}, Avg: {sum(tool_counts)/len(tool_counts):.1f}")
            print()

            if tools_used:
                print(f"   Top tools used:")
                for tool_name, count in sorted(tools_used.items(), key=lambda x: -x[1])[:5]:
                    print(f"     - {tool_name}: {count} times")
        print()

    def analyze_timing(self):
        """Analyze request timing to detect latency patterns"""
        print("=" * 100)
        print("TIMING ANALYSIS")
        print("=" * 100)
        print()

        if len(self.traces) < 2:
            print("Need at least 2 traces for timing analysis")
            return

        print("ðŸ“… Request Timeline:")
        print()

        for i, trace in enumerate(self.traces[-10:]):  # Last 10 requests
            timing = self.get_timing_info(trace)
            req_body = trace.get("request", {}).get("body", {})
            msg_count = len(req_body.get("messages", []))
            tool_count = len(req_body.get("tools", []))
            finish_reason = trace.get("response", {}).get("body", {}).get("choices", [{}])[0].get("finish_reason", "?")

            print(f"  [{i}] {timing['timestamp']} | " +
                  f"Msg: {msg_count:2d} | Tool: {tool_count:2d} | " +
                  f"Finish: {finish_reason:20s}")

        print()
        print("ðŸ’¡ To measure cache effectiveness:")
        print("   Look for requests with identical messages + tools at different timestamps")
        print("   If timing/processing is similar, cache is NOT working")
        print("   If second request is much faster, cache IS working")
        print()

    def generate_benchmark_commands(self):
        """Suggest commands to benchmark cache effectiveness"""
        print("=" * 100)
        print("HOW TO BENCHMARK CACHE EFFECTIVENESS")
        print("=" * 100)
        print()

        print("Current approach (using existing traces):")
        print("  1. Find identical requests in your trace history")
        print("  2. Compare their response times")
        print("  3. If 2nd request is much faster â†’ cache is working")
        print()

        print("Active benchmarking (send test requests):")
        print("  # Start server with debug logging")
        print("  PROXY_ONLY=true ANYCLAUDE_DEBUG=1 anyclaude --mode=mlx &")
        print()
        print("  # Run same request twice to test cache")
        print("  curl -X POST http://localhost:8081/v1/chat/completions \\")
        print('    -H "Content-Type: application/json" \\')
        print('    -d \'{"messages": [{"role": "user", "content": "What is 2+2?"}], "stream": false}\'')
        print()
        print("  # Measure second request latency (should be much faster)")
        print()

        print("Extract timing from server logs:")
        print("  # Check MLX server output for timing info")
        print("  curl http://localhost:8081/health | jq '.cache'")
        print()

        print("Analyze all traces:")
        print("  # Get raw request/response data")
        print("  python3 scripts/analyze-vllm-traces.py")
        print()

    def run(self):
        """Run all analysis"""
        if not self.traces:
            print("âŒ No trace files found!")
            print(f"Expected location: {self.trace_dir}")
            return

        self.analyze_cache_effectiveness()
        self.analyze_request_patterns()
        self.analyze_timing()
        self.generate_benchmark_commands()

if __name__ == "__main__":
    trace_dir = sys.argv[1] if len(sys.argv) > 1 else None
    analyzer = TraceAnalyzer(trace_dir)
    analyzer.run()
