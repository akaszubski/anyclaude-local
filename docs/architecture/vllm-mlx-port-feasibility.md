# vLLM + MLX Port Feasibility Analysis

**Date**: 2025-11-21
**Question**: Can we port vLLM to use MLX as a backend for Apple Silicon?

**Short Answer**: âœ… **Partially YES!** The path exists but requires significant work. PagedAttention kernels already exist for Metal!

---

## Executive Summary

**The Good News** ğŸ‰:

1. âœ… **PagedAttention kernels for Metal already exist** (Eric Buehler, 2025)
2. âœ… **77-131% throughput improvements demonstrated** on M3 Max
3. âœ… **MLX's unified memory architecture is ideal** for PagedAttention
4. âœ… **Community interest is high** (multiple issues, active development)

**The Challenges** âš ï¸:

1. âŒ vLLM is deeply tied to CUDA/HIP (thousands of GPU kernel calls)
2. âŒ No official Apple Silicon support planned by vLLM team
3. âš ï¸ Would require massive rewrite (~50-80% of codebase)
4. âš ï¸ Better approach: **Build vLLM-like features into MLX-LM**

**Recommendation**: Don't port vLLM â†’ MLX. Instead, **add vLLM features to MLX-LM** using existing PagedAttention kernels!

---

## Part 1: Current State of vLLM on macOS

### What Works

- âœ… **CPU-only mode** (experimental, from source)
- âœ… **Basic inference** without GPU acceleration
- âš ï¸ **Very slow** (10-50x slower than CUDA)

### What Doesn't Work

- âŒ **Metal/MPS backend** (not implemented)
- âŒ **GPU acceleration** on Apple Silicon
- âŒ **PagedAttention** (CUDA-only)
- âŒ **Continuous batching** (CUDA-only)
- âŒ **FlashAttention** (CUDA kernels)

### Why vLLM Doesn't Support Apple Silicon

**Technical Blockers**:

1. **CUDA Dependency**: vLLM's core is built on CUDA graphs and kernels
2. **FlashAttention**: CUDA-specific implementation (no Metal port)
3. **PagedAttention**: Original implementation uses CUDA unified memory
4. **Kernel Fusion**: Thousands of optimized CUDA kernels throughout codebase

**Organizational**:

- GitHub Issue #16653 closed as "NOT_PLANNED" (April 2025)
- Community requests stale after 90 days
- No official support from vLLM maintainers

---

## Part 2: The PagedAttention Breakthrough! ğŸš€

### What Is PagedAttention?

PagedAttention is vLLM's **killer feature** - the algorithm that enables:

- ğŸ“‰ 2-4x lower memory usage (vs traditional attention)
- ğŸ“ˆ 10-100x higher throughput (continuous batching)
- ğŸ”„ Dynamic memory allocation (no pre-allocation needed)

**How it works**:

```
Traditional Attention:
Request 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (allocates full KV cache upfront)
Request 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (even if only uses 30%)
Request 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â†’ Huge memory waste, limited concurrency

PagedAttention:
Request 1: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (allocates blocks as needed)
Request 2: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (shares freed blocks)
Request 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘
â†’ 2-4x more requests fit in same memory!
```

### MLX PagedAttention Kernels (July 2025)

**Who**: Eric Buehler (Rust MLX inference engine creator)

**What**: Metal-optimized PagedAttention kernels for MLX

**Performance** (mistralrs-server vs llama.cpp on M3 Max):

| Model                    | llama.cpp   | mistralrs (MLX+PagedAttn) | Improvement    |
| ------------------------ | ----------- | ------------------------- | -------------- |
| **Qwen 3 30B (4-bit)**   | 9.24 tok/s  | 16.34 tok/s               | **+77%** ğŸ”¥    |
| **Llama 3.2 3B (8-bit)** | 10.08 tok/s | 23.28 tok/s               | **+131%** ğŸ”¥ğŸ”¥ |

**Status**:

- âœ… Kernels implemented and working
- âš ï¸ Not integrated into MLX core library
- ğŸ—ï¸ Available via community kernels project (Hugging Face)

---

## Part 3: Why MLX Is Perfect for PagedAttention

### Apple's Unified Memory Architecture

**Traditional GPUs** (NVIDIA/AMD):

```
CPU Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â†“ (PCIe transfer)
GPU Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    â†’ Expensive copies, limited bandwidth
```

**Apple Silicon** (M1/M2/M3/M4):

```
Unified Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
     â†“              â†“
    CPU           GPU
    â†’ Zero-copy, shared address space!
```

**Why This Matters for PagedAttention**:

1. **No Transfer Overhead**: KV cache blocks can be accessed by both CPU (scheduler) and GPU (inference) without copying
2. **Dynamic Allocation**: MLX can allocate/free blocks instantly (no GPU malloc delays)
3. **Memory Pressure Handling**: Apple's memory controller optimizes page swapping
4. **Large Memory Pools**: Mac Studio M3 Ultra has 192GB unified memory (vs 80GB NVIDIA A100!)

### MLX's Design Philosophy

MLX was **built for unified memory** from day one:

```python
# MLX arrays live in shared memory
import mlx.core as mx

# Create array - accessible to both CPU and GPU
x = mx.array([1, 2, 3])  # Lives in unified memory

# CPU operation - no transfer needed
y = x + 1  # Runs on CPU

# GPU operation - no transfer needed
z = mx.matmul(x, x.T)  # Runs on GPU

# Both see same memory!
```

**vLLM's PagedAttention on CUDA**:

- Must manage CPU â†” GPU transfers
- Block table lives in CPU memory
- KV blocks live in GPU memory
- Expensive synchronization overhead

**PagedAttention on MLX**:

- Block table and KV blocks share unified memory
- Zero transfer overhead
- Simpler implementation
- Potentially **faster than vLLM**!

---

## Part 4: Feasibility Analysis - Porting vLLM to MLX

### Approach 1: Full vLLM Port (âŒ Not Recommended)

**Effort**: 12-24 months, 2-3 full-time engineers

**What Needs Porting**:

| Component               | CUDA Code            | Metal Port Difficulty    | Estimated Effort |
| ----------------------- | -------------------- | ------------------------ | ---------------- |
| PagedAttention          | âœ… Exists (Eric B.)  | ğŸŸ¢ Done                  | 0 weeks          |
| FlashAttention          | CUDA kernels         | ğŸ”´ Very Hard             | 12-16 weeks      |
| CUDA Graphs             | Graph capture/replay | ğŸ”´ Metal doesn't support | N/A (use alt)    |
| KV Cache Manager        | CUDA malloc          | ğŸŸ¡ Medium                | 4-6 weeks        |
| Scheduler               | CUDA streams         | ğŸŸ¡ Medium                | 6-8 weeks        |
| Model Execution         | CUDA kernels         | ğŸŸ¢ Easy (MLX has)        | 2-3 weeks        |
| Continuous Batching     | CUDA async           | ğŸŸ¡ Medium                | 8-10 weeks       |
| Quantization (GPTQ/AWQ) | CUDA kernels         | ğŸ”´ Hard                  | 10-12 weeks      |
| **Total**               | -                    | -                        | **42-55 weeks**  |

**Blockers**:

1. **CUDA Graphs**: Metal doesn't have direct equivalent
2. **FlashAttention**: Would need complete Metal rewrite
3. **Kernel Fusion**: Thousands of small optimizations to port
4. **Maintenance Burden**: vLLM updates constantly, keeping fork in sync is brutal

**Verdict**: âŒ **Not worth the effort.** Too much work, high maintenance burden.

---

### Approach 2: Add vLLM Features to MLX-LM (âœ… Recommended)

**Effort**: 3-6 months, 1 engineer

**What To Build**:

| Feature                 | Complexity | Uses Existing       | Estimated Effort |
| ----------------------- | ---------- | ------------------- | ---------------- |
| **PagedAttention**      | ğŸŸ¢ Easy    | âœ… Eric's kernels   | 2-3 weeks        |
| **Block Manager**       | ğŸŸ¡ Medium  | New code            | 3-4 weeks        |
| **FIFO Scheduler**      | ğŸŸ¢ Easy    | New code            | 2-3 weeks        |
| **Continuous Batching** | ğŸŸ¡ Medium  | MLX batch support   | 4-6 weeks        |
| **Request Queue**       | ğŸŸ¢ Easy    | AsyncIO             | 1-2 weeks        |
| **KV Cache Sharing**    | ğŸŸ¡ Medium  | Our ram_cache.py    | 2-3 weeks        |
| **Grammar Constraints** | ğŸ”´ Hard    | Port xgrammar?      | 8-12 weeks       |
| **Tool Call Stops**     | ğŸŸ¢ Easy    | Our truncation code | 1 week           |
| **Total**               | -          | -                   | **23-34 weeks**  |

**Advantages**:

- âœ… **Leverage existing MLX ecosystem** (no CUDA porting)
- âœ… **Use proven Metal kernels** (PagedAttention already works)
- âœ… **Smaller scope** (~40% effort vs full port)
- âœ… **Can integrate incrementally** (ship features one by one)
- âœ… **Native MLX performance** (no compatibility layers)

**Verdict**: âœ… **This is the path forward!**

---

## Part 5: Concrete Roadmap - Building "vLLM-for-MLX"

### Phase 1: PagedAttention Foundation (4-6 weeks)

**Goal**: Integrate Eric Buehler's PagedAttention kernels into MLX-LM

**Tasks**:

1. Import PagedAttention kernels from community repo
2. Implement block table manager (allocate/free KV blocks)
3. Update `mlx_lm.generate()` to use paged KV cache
4. Add block scheduling logic (FIFO to start)

**Success Metrics**:

- Single-request inference works with PagedAttention
- Memory usage 2-4x lower than current MLX-LM
- Performance matches or beats current implementation

**Code Location**: `scripts/mlx-server.py` â†’ new `paged_attention.py` module

---

### Phase 2: Continuous Batching (6-8 weeks)

**Goal**: Process multiple requests concurrently (like vLLM)

**Tasks**:

1. Implement request queue with AsyncIO
2. Add FIFO scheduler (batches pending requests)
3. Support dynamic batch sizes (add/remove requests mid-batch)
4. Handle variable sequence lengths with padding/masking

**Success Metrics**:

- Process 5-10 concurrent requests
- Throughput 5-10x higher than sequential
- No attention mask bugs (solve MLX-LM Issue #178!)

**Challenges**:

- MLX batch attention has bugs (see Issue #178)
- Need proper mask handling with padding
- Dynamic batch reshaping is tricky

---

### Phase 3: Grammar Constraints (8-12 weeks)

**Goal**: Prevent infinite loops, ensure valid JSON (our bug!)

**Tasks**:

1. Research xgrammar Metal port feasibility
2. If infeasible, build lightweight JSON validator
3. Integrate grammar checks into sampling loop
4. Add `tool_choice` parameter support

**Success Metrics**:

- Guaranteed valid tool calls (no infinite loops!)
- JSON schema validation for outputs
- `tool_choice="required"` works like vLLM

**Note**: This is the **hardest part** - may need to simplify if xgrammar port is too complex.

---

### Phase 4: Production Hardening (4-6 weeks)

**Goal**: Make it production-ready

**Tasks**:

1. Add metrics/monitoring (request latency, throughput, etc.)
2. Error handling and recovery (OOM, timeout, etc.)
3. Request prioritization (not just FIFO)
4. Memory pressure handling (evict blocks under pressure)
5. Multi-model support (load balancing across models)

**Success Metrics**:

- Handles 50+ concurrent requests
- Graceful degradation under load
- Auto-recovery from errors

---

## Part 6: Performance Projections

Based on Eric Buehler's benchmarks + vLLM's reported improvements:

### Current State (Your MLX Setup)

| Metric                  | Current Performance     |
| ----------------------- | ----------------------- |
| **Single Request**      | ~10-15 tok/s (Qwen 30B) |
| **Concurrent Requests** | 1 (sequential only)     |
| **Throughput**          | 10-15 tok/s total       |
| **Memory Efficiency**   | ~40GB for 30B model     |

### After Phase 1 (PagedAttention)

| Metric                  | Projected Performance | Improvement      |
| ----------------------- | --------------------- | ---------------- |
| **Single Request**      | ~15-20 tok/s          | +30-50%          |
| **Concurrent Requests** | 1 (still sequential)  | 0%               |
| **Throughput**          | 15-20 tok/s total     | +30-50%          |
| **Memory Efficiency**   | ~15-20GB for 30B      | **2-3x better!** |

### After Phase 2 (Continuous Batching)

| Metric                  | Projected Performance   | Improvement |
| ----------------------- | ----------------------- | ----------- |
| **Single Request**      | ~15-20 tok/s            | +30-50%     |
| **Concurrent Requests** | 5-10                    | **10x!**    |
| **Throughput**          | **150-200 tok/s total** | **10-15x!** |
| **Memory Efficiency**   | ~15-20GB for 30B        | 2-3x better |

### After Phase 3 (Grammar Constraints)

| Metric                | Projected Performance        | Improvement        |
| --------------------- | ---------------------------- | ------------------ |
| **Tool Call Quality** | 100% valid JSON              | âˆ (no more loops!) |
| **Latency**           | Slightly slower (validation) | -5-10%             |
| **Reliability**       | Production-grade             | **Huge!**          |

---

## Part 7: Alternative: Use mistralrs-server

**What Is It?**: Rust-based inference server with MLX backend + PagedAttention (by Eric Buehler)

**Advantages**:

- âœ… **Already has PagedAttention** (+77-131% throughput)
- âœ… **Continuous batching** support (`--max-seqs` parameter)
- âœ… **OpenAI-compatible API** (drop-in replacement)
- âœ… **Metal optimized** (native performance)
- âœ… **Maintained** (active development)

**Disadvantages**:

- âš ï¸ Rust codebase (harder to customize than Python)
- âš ï¸ Less mature than vLLM (newer project)
- âš ï¸ Smaller community (vs vLLM/MLX)

**Integration with anyclaude**:

```json
// .anyclauderc.json
{
  "backend": "mistralrs",
  "backends": {
    "mistralrs": {
      "enabled": true,
      "baseUrl": "http://localhost:8080/v1",
      "model": "qwen-30b-mlx",
      "apiKey": "mistralrs"
    }
  }
}
```

**Recommendation**: â­ **Try mistralrs-server as a stopgap** while building vLLM features into MLX-LM!

---

## Part 8: Comparison Matrix

| Feature                 | vLLM (CUDA)    | mistralrs (MLX)  | Our MLX Setup    | vLLM-MLX (Proposed) |
| ----------------------- | -------------- | ---------------- | ---------------- | ------------------- |
| **PagedAttention**      | ğŸŸ¢ Yes         | ğŸŸ¢ Yes           | ğŸ”´ No            | ğŸŸ¢ Yes (Phase 1)    |
| **Continuous Batching** | ğŸŸ¢ Yes         | ğŸŸ¢ Yes           | ğŸ”´ No            | ğŸŸ¢ Yes (Phase 2)    |
| **Grammar Constraints** | ğŸŸ¢ xgrammar    | ğŸŸ¡ Limited       | ğŸŸ¡ Truncation    | ğŸŸ¢ Yes (Phase 3)    |
| **Tool Call Stops**     | ğŸŸ¢ Native      | ğŸŸ¡ Basic         | ğŸŸ¡ Workaround    | ğŸŸ¢ Native (Phase 3) |
| **RAM KV Cache**        | ğŸ”´ No          | ğŸ”´ No            | ğŸŸ¢ Yes (200x!)   | ğŸŸ¢ Yes (keep ours!) |
| **Multi-GPU**           | ğŸŸ¢ Yes         | ğŸ”´ No            | ğŸ”´ No            | ğŸ”´ No (Apple limit) |
| **Throughput**          | ğŸŸ¢ 500+ tok/s  | ğŸŸ¡ 20-50 tok/s   | ğŸŸ¡ 10-15 tok/s   | ğŸŸ¢ 150-200 tok/s    |
| **Platform**            | ğŸŸ¡ NVIDIA only | ğŸŸ¢ Apple Silicon | ğŸŸ¢ Apple Silicon | ğŸŸ¢ Apple Silicon    |
| **Maintenance**         | ğŸŸ¢ Active      | ğŸŸ¢ Active        | ğŸŸ¡ DIY           | ğŸŸ¡ DIY              |

---

## Part 9: Decision Framework

### When to Port vLLM â†’ MLX Backend

**Do This If**:

- âŒ You need 100% vLLM API compatibility
- âŒ You want to run exact same code on CUDA + Metal
- âŒ You have 1+ year and 2-3 engineers

**Don't Do This** (our case!)

---

### When to Add vLLM Features to MLX-LM

**Do This If**:

- âœ… You want vLLM-like performance on Apple Silicon
- âœ… You're willing to invest 3-6 months of development
- âœ… You can leverage existing PagedAttention kernels
- âœ… You want to contribute to MLX ecosystem

**This is us!** ğŸ¯

---

### When to Use mistralrs-server

**Do This If**:

- âœ… You need PagedAttention **today**
- âœ… You're okay with Rust codebase
- âœ… You want +77-131% throughput immediately
- âœ… You can live with less customization

**Try this as interim solution!**

---

## Part 10: Recommendations

### Immediate Actions (Next 2 Weeks)

1. âœ… **Test mistralrs-server** with your models

   ```bash
   # Install mistralrs
   cargo install mistralrs-server

   # Run with MLX backend
   mistralrs-server --port 8080 \
     --model qwen-30b-mlx \
     --backend mlx \
     --max-seqs 10

   # Test with anyclaude
   anyclaude --mode=mistralrs
   ```

2. âœ… **Benchmark performance**
   - Compare mistralrs vs your current MLX setup
   - Measure throughput with 1, 5, 10 concurrent requests
   - Check memory usage

3. âœ… **Evaluate trade-offs**
   - Does mistralrs meet your needs?
   - Or do you need custom features (RAM cache, etc.)?

### Short-Term (1-3 Months)

**If mistralrs works**: âœ… Use it! Focus on other features.

**If mistralrs doesn't work**:

1. Start **Phase 1: PagedAttention Integration**
2. Import Eric Buehler's kernels
3. Build block manager for MLX-LM
4. Measure performance improvements

### Long-Term (6-12 Months)

**If building vLLM-MLX**:

- Complete Phases 1-4 (PagedAttention â†’ Grammar)
- Publish as open-source MLX-LM enhancement
- Contribute upstream to ml-explore/mlx-lm

**If using mistralrs**:

- Contribute improvements upstream
- Focus on anyclaude features (debugging, caching, etc.)
- Monitor MLX-LM for native PagedAttention support

---

## Conclusion

**Can we port vLLM to use MLX?**

**Technical Answer**: Yes, but it's the wrong approach.

**Better Answer**: âœ… **Build vLLM-like features into MLX-LM using existing PagedAttention kernels.**

**Best Answer**: ğŸ¯ **Try mistralrs-server first (already done!), then build vLLM-MLX if needed.**

---

## Next Steps

1. â­ï¸ **Try mistralrs-server** (2-3 days)
2. â­ï¸ **Document results** in comparison guide
3. â­ï¸ **Decide on approach**:
   - Stick with mistralrs? â†’ Focus elsewhere
   - Need custom solution? â†’ Start Phase 1

---

## References

- vLLM GitHub: https://github.com/vllm-project/vllm
- MLX PagedAttention Issue: https://github.com/ml-explore/mlx/issues/2228
- mistralrs-server: https://github.com/EricLBuehler/mistral.rs
- Eric Buehler's benchmarks: See MLX Issue #2228
- PagedAttention paper: https://arxiv.org/abs/2309.06180
