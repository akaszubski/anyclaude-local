# AnyClaude Project Documentation

## Project Vision

**Make Claude Code work seamlessly with any AI backend - local, cloud, or hybrid.**

AnyClaude is a translation layer that bridges the gap between Claude Code (Anthropic's official CLI tool) and multiple AI providers. Whether you're using local models (mistral.rs with MLX, LMStudio), cloud models (OpenRouter with 400+ options), or official Claude API (Max subscription or API key), anyclaude provides a unified, flexible development experience optimized for your needs.

## GOALS

### Primary Goals

1. **Enable Privacy-First Development**
   - Run Claude Code completely offline with local models (mistral.rs with MLX, LMStudio)
   - Zero data transmission to cloud services
   - Full control over code and conversations
   - Support for Apple Silicon (M1/M2/M3/M4) with native MLX acceleration

2. **Reduce AI Development Costs**
   - Free: Local models (mistral.rs, LMStudio) with no API costs
   - 84% savings: OpenRouter ($0.60-$2/1M tokens vs Claude $3-$15/1M)
   - Flexible: Switch modes based on task requirements
   - Efficient: Prompt caching reduces token usage by 30-50%

3. **Seamless Claude Code Experience**
   - Full tool calling support (Read, Write, Edit, Bash, Git, etc.)
   - Production-ready with mistral.rs inference engine
   - Streaming responses with proper backpressure handling
   - Authentication compatibility (Claude Max + API keys)
   - Hot-swappable models without restart

4. **Developer Productivity**
   - Auto-launch mistral.rs server for zero-config experience
   - Native MLX acceleration with KV caching
   - Mode switching via CLI flag, env var, or config file
   - Comprehensive debug logging (3 levels) for troubleshooting

### Success Metrics

- ‚úÖ **Functionality**: Tool calling works 100% (4/4 backends: mistral.rs/OpenRouter/LMStudio/Claude all work)
- ‚úÖ **Performance**: 60-85% cache hit rate, 30-50% token reduction; RAM cache provides 100-200x latency improvement for M3 Ultra
- ‚úÖ **Quality**: **1,400+ tests across 101 test files** (unit, integration, regression, E2E, performance)
  - Core translation: 180+ tests
  - Error handling: 150+ tests (10 suites √ó 10 tests each)
  - Production hardening: 151 tests (error recovery, metrics, config validation)
  - Cache systems: 141 tests (84 cache control + 57 RAM cache)
  - Tool calling: 145+ tests (parser plugins, circuit breaker, schema validation)
  - Streaming: 37 tests (JSON parser, 78% complete)
  - Integration: 300+ tests (message pipeline, tool workflow, proxy cycle, E2E conversations)
- üéØ **User Adoption**: Enable 1000+ developers to use local models with Claude Code
- üéØ **Cost Savings**: Help users save $100-1000/month on AI API costs

## SCOPE

### IN SCOPE

**Core Functionality**:

- ‚úÖ Translation between Anthropic Messages API and OpenAI Chat Completions format
- ‚úÖ Support for 4 backend modes: mlx (mistral.rs), lmstudio, openrouter, claude
- ‚úÖ Full tool calling translation (streaming and atomic formats)
- ‚úÖ Streaming response adaptation (AI SDK ‚Üí Anthropic SSE)
- ‚úÖ Context window management with automatic truncation
- ‚úÖ Trace logging for cloud modes (auto-enabled, API keys redacted)
- ‚úÖ **Native MLX acceleration with mistral.rs**: Production-ready Rust inference engine with KV caching, MoE support, and excellent tool calling

**Production Infrastructure**:

- ‚úÖ **Error Recovery**: Graceful degradation, circuit breaker (CLOSED/OPEN/HALF_OPEN states), failover manager with exponential backoff
- ‚úÖ **Monitoring**: Telemetry collector, cache monitor, metrics endpoint (`/v1/metrics`), JSON/Prometheus export
- ‚úÖ **Validation**: Config validator with pre-startup checks, schema validator for tool definitions, dependency verification
- ‚úÖ **Tool Parsing**: Plugin system with 6 parsers (Qwen, GPT-OSS, Hermes, Anthropic, Generic, Fallback), automatic parser selection
- ‚úÖ **Streaming Optimization**: JSON parser with incremental parsing, early tool detection (60% faster), character-by-character tokenization
- ‚úÖ **Cache Control**: Anthropic cache_control marker extraction, SHA256 hash generation, zero-overhead when disabled
- ‚úÖ **Health Monitoring**: Server health checks, timeout detection, process management, OOM detection

**Advanced Features**:

- ‚úÖ **Streaming JSON Parser** (693 lines): State machine with incremental parsing, 1MB buffer limit, 64-level nesting protection, 30s timeout
- ‚úÖ **Cache Control System** (128 lines): SHA256 hashing, token estimation, cache marker extraction, performance metrics
- ‚úÖ **Tool Parser Plugins** (561 lines): 6 model-specific parsers, automatic fallback chains, error recovery
- ‚úÖ **Circuit Breaker** (230 lines): Failure tracking, automatic recovery, half-open testing, configurable thresholds
- ‚úÖ **Model Adapters Framework**: Per-model schema simplification, parameter validation, tool selection guidance
- ‚úÖ **Trace Analysis Tools**: CLI analyzer for token breakdown, trace replayer for benchmarking, comparison tools

**Supported Platforms**:

- ‚úÖ macOS (Apple Silicon and Intel)
- ‚úÖ Linux (tested with LMStudio)
- ‚ö†Ô∏è Windows (should work, community-tested)

**Supported Models** (verified working):

- ‚úÖ Qwen3 Coder 30B, GPT-OSS 20B, DeepSeek Coder
- ‚úÖ Mistral, Llama variants with tool calling support
- ‚úÖ Any MLX-quantized model (4-bit, 6-bit, 8-bit)
- ‚úÖ OpenRouter: GLM-4.6, Qwen 2.5 72B, 400+ others

**Testing & Quality**:

- ‚úÖ **1,400+ automated tests across 101 test files** (unit, integration, regression, E2E, performance)
- ‚úÖ **Git hooks** (pre-commit: fast checks, pre-push: full suite runs automatically)
- ‚úÖ **Regression prevention** (streaming bugs, context issues, tool calling failures caught before push)
- ‚úÖ **Security hardening** (DoS prevention, input validation, memory safety, path traversal protection)
- ‚úÖ **Error recovery testing** (150+ tests for file I/O, network, tool validation, config, message conversion)
- ‚úÖ **Performance validation** (benchmarking, load testing, concurrent requests, large context handling)

### OUT OF SCOPE

**Will NOT Support**:

- ‚ùå Multi-cloud provider orchestration (use original anyclaude for this)
- ‚ùå Complex failover systems (removed for simplicity)
- ‚ùå GPT-5 specific features (reasoning controls, service tiers)
- ‚ùå Non-OpenAI-compatible servers (e.g., native HuggingFace API)
- ‚ùå GUI configuration tools (CLI-first approach)

**Explicitly NOT Goals**:

- ‚ùå Replicating Claude's intelligence (we translate, not replace)
- ‚ùå Supporting every LLM format (focus on OpenAI-compatible)
- ‚ùå Building a model marketplace (use LMStudio/HF for discovery)
- ‚ùå Cloud-hosted proxy service (local-first philosophy)

### Future Considerations

**May Add Later** (based on user demand):

- üîÑ Ollama support (if users request cross-platform alternatives)
- üîÑ Enhanced model adapters (per-model schema optimizations)
- üîÑ Response quality improvements (retry logic, formatting fixes)
- üîÑ Performance optimizations (connection pooling, compression)
- üîÑ Additional authentication methods (OAuth, custom headers)

**Community Contributions Welcome**:

- Testing on Windows platform
- Support for additional local model servers
- Model-specific prompt templates
- Documentation improvements

## CONSTRAINTS

### Technical Constraints

**Hardware Requirements**:

- **Apple Silicon (MLX-Textgen)**: M1/M2/M3/M4 with 16GB+ RAM recommended
  - 32GB+ for 30B models, 64GB+ for best performance
  - GPU cores impact speed (more cores = faster inference)
- **Intel/AMD (LMStudio)**: 8GB+ VRAM for GPU acceleration
  - Larger models require more VRAM or CPU-only mode (slower)

**Context Window Limits**:

- Local models: 8K-128K tokens (model-dependent)
- Claude/OpenRouter: Up to 200K tokens
- System prompts from Claude Code: ~18,500 tokens per request
- Automatic truncation when limits exceeded

**Performance Expectations**:

- First request: 3-20 seconds (MLX-Textgen with KV cache)
- Follow-ups (with cache): 0.5-10 seconds (MLX-Textgen) or 25-35 seconds (LMStudio)
- Token generation: 2-8 tokens/sec (hardware-dependent)
- **Note**: MLX-Textgen provides 10-90x speedup on follow-ups via disk-based KV caching

**Model Compatibility**:

- Must support OpenAI Chat Completions format
- Tool calling requires function calling support in model
- Not all LMStudio models work - verify tool calling before use

### Architectural Constraints

**Single Translation Layer**:

- Supports ONE backend mode at a time per process
- Switch modes by restarting with different `ANYCLAUDE_MODE`
- No automatic failover between backends (by design - simplicity)

**Dependency on External Servers**:

- MLX-Textgen: Python 3.12+, mlx-textgen pip package, Rust toolchain
- LMStudio: Requires GUI app running on port 1234
- OpenRouter: Internet connection and API key
- Cannot work offline in cloud modes

**TypeScript/Node.js Platform**:

- Requires Node.js 18+ for runtime
- Bun for building (faster than npm)
- Cannot be compiled to native binary (interpreter needed)

### Development Constraints

**Testing Limitations**:

- Full E2E tests require running Claude Code (manual)
- Cannot mock LMStudio server realistically (integration tests limited)
- Hardware-dependent performance tests (inconsistent across machines)

**Documentation Maintenance**:

- README.md must stay in sync with PROJECT.md
- Breaking changes require updating 15+ doc files
- Manual sync (no automation for doc consistency yet)

**Compatibility Boundaries**:

- Tightly coupled to Claude Code 2.0 format
- Breaking changes in Anthropic API require immediate updates
- LMStudio API changes may break compatibility

### Resource Constraints

**Disk Space**:

- Models: 2-30GB per MLX model (depending on size and quantization)
- Traces: Can accumulate to several GB (auto-saved to `~/.anyclaude/traces/`)
- Logs: Debug mode generates large log files

**Network**:

- OpenRouter mode: ~20-50KB per request/response
- Model downloads: 2-30GB for initial MLX model download
- MLX-Textgen: No network after model download (fully offline)

**Time Investment**:

- Initial setup: 5-30 minutes (including model download)
- Per-session overhead: 30-50 seconds for first request
- Mode switching: Instant (just restart with env var)

### Security Constraints

**Privacy Guarantees**:

- ‚úÖ Local modes (MLX-Textgen, LMStudio): No data leaves machine
- ‚ö†Ô∏è Cloud modes (OpenRouter, Claude): Data sent to third parties
- ‚úÖ Trace files: API keys auto-redacted
- ‚ùå No encryption for local trace files (trust user's OS security)

**Authentication Limitations**:

- Bearer tokens passed through transparently (cannot validate)
- API keys not validated before sending to backend
- No rate limiting or quota management

**Code Execution Risk**:

- Bash tool allows arbitrary command execution (trust model output)
- Write/Edit tools allow file system modifications
- No sandboxing of tool calls (inherits Claude Code's trust model)

## Origins: A Port for Privacy

This project is a **port of the original anyclaude** concept, reimagined specifically for **Claude Code 2.0** compatibility. The goal is to enable developers to:

1. **Choose their backend** - local models (MLX-Textgen, LMStudio), cheap cloud (OpenRouter), or official Claude API
2. **Maintain privacy** when needed by keeping code and conversations on-device with local models
3. **Control costs** - from free (local) to 84% cheaper than Claude API (OpenRouter) to premium (Claude)
4. **Work seamlessly** - OpenRouter/LMStudio/Claude work with full tool calling (MLX-Textgen tool calling currently broken)
5. **Switch dynamically** - change modes via CLI flag, env var, or config file

### Why Claude Code 2.0?

Claude Code represents Anthropic's vision for AI-powered development - sophisticated tool use, multi-step reasoning, and seamless integration with development workflows. However, the official version requires:

- **Claude Max subscription** ($20-200/month) with session-based auth, OR
- **Claude API access** ($3/$15 per 1M tokens) with pay-per-token pricing

AnyClaude expands your options with **four backend modes**:

1. **MLX-Textgen** (mlx-textgen mode) - Free local models on Apple Silicon with auto-launch and 10-90x KV cache speedup
2. **LMStudio** - Free local models, cross-platform, manual server management
3. **OpenRouter** - Cloud models at fraction of Claude cost ($0.60/$2 per 1M for GLM-4.6 = 84% cheaper)
4. **Claude API** - Official Anthropic API with trace logging for reverse engineering

**Benefits by mode**:

- **MLX-Textgen**: 100% privacy, no API costs, 10-90x speedup via KV cache (‚ö†Ô∏è tool calling broken, Q&A only)
- **LMStudio**: 100% privacy, cross-platform, tool calling works
- **OpenRouter**: 400+ models, Claude-like quality at 84% lower cost, tool calling works
- **Claude API**: Highest quality, full feature support, automatic trace logging

## Core Principle: Active Translation, Not Simple Passthrough

While anyclaude acts as an HTTP proxy server, its primary role is **intelligent translation and adaptation**:

### What Makes It More Than a Proxy

1. **Bidirectional Format Translation**
   - Claude Code sends: Anthropic Messages API format
   - Local models (MLX-Textgen/LMStudio) expect: OpenAI Chat Completions format
   - We translate in both directions, preserving semantics
   - OpenRouter/Claude: Native format passthrough (both use Anthropic-compatible API)

2. **Streaming Protocol Adaptation**
   - Claude Code expects: Server-Sent Events (SSE) with specific event types
   - Local models send: OpenAI streaming format via AI SDK
   - We convert chunk-by-chunk in real-time with backpressure handling
   - OpenRouter/Claude: Native SSE streaming

3. **Tool Call Translation**
   - Claude Code uses: Atomic tool calls (complete parameters at once)
   - AI SDK sends: Streaming tool parameters (progressive updates)
   - We buffer and consolidate to match expectations
   - Deduplicates redundant tool-call events

4. **Authentication Flexibility**
   - Claude Max: Session-based Bearer tokens (passthrough)
   - Claude API: API keys in headers (passthrough)
   - OpenRouter: API key with custom headers (HTTP-Referer, X-Title)
   - Local models: No authentication required

5. **Context Window Management**
   - Claude/OpenRouter: Up to 200K tokens (model-dependent)
   - Local models: Often 8K-128K tokens
   - We automatically estimate and track token usage
   - Context manager handles overflow gracefully

6. **Trace Logging & Analysis**
   - Auto-enabled for cloud modes (claude, openrouter)
   - Records full request/response pairs with API keys redacted
   - Enables reverse-engineering Claude Code's prompting patterns
   - Saved to `~/.anyclaude/traces/{mode}/` for analysis

## Architecture

### Four-Mode Design

anyclaude supports **four backend modes**, each optimized for different use cases:

#### 1. MLX Mode (`ANYCLAUDE_MODE=mlx`, default)

**Purpose**: High-performance local inference on Apple Silicon with custom KV caching and vLLM-inspired features

**Status**: ‚úÖ Tool calling WORKS with custom mlx-server.py implementation

**Features**:

- Auto-launches custom MLX server (scripts/mlx-server.py) when model path is configured
- Routes to local server (default: `http://localhost:8080/v1`)
- Native MLX acceleration for M1/M2/M3/M4 chips
- **RAM-based KV cache** for 100-200x speedup (<1ms GET vs 500-2000ms disk)
- **Full tool calling support** (Read, Write, Edit, Bash, Git, etc.)
- Full translation layer (Anthropic format ‚Üí OpenAI format)
- **vLLM-inspired features**: Error recovery, circuit breaker, metrics monitoring
- Production hardening: OOM detection, graceful degradation, health checks
- Auto-cleanup when Claude Code exits

**Architecture**:

```
Claude Code ‚Üí AnyClaude Proxy ‚Üí Custom MLX Server ‚Üí MLX Model
(Anthropic API)  (Translation)   (scripts/mlx-server.py)  (Local file)
                                  ‚Üì
                                  RAM KV Cache (100-200x faster)
                                  ‚Üì
                                  vLLM-inspired features
```

**Use Cases**:

- Privacy-first development (100% local, no cloud)
- Cost-free unlimited queries
- Offline development
- **Fast coding with tool calling**: Read/Write/Edit/Bash all work
- Ultra-fast follow-ups with RAM cache (100-200x speedup)
- Production-grade reliability with error recovery

**Configuration**:

```json
{
  "backend": "mlx",
  "backends": {
    "mlx": {
      "enabled": true,
      "port": 8080,
      "model": "/path/to/your/mlx/model",
      "serverScript": "scripts/mlx-server.py"
    }
  }
}
```

---

#### 2. LMStudio Mode (`ANYCLAUDE_MODE=lmstudio`)

**Purpose**: Cross-platform local inference with GUI model management

**Features**:

- Manual server management via LMStudio GUI
- Routes to LMStudio server (default: `http://localhost:8082/v1`)
- Model-agnostic: works with whatever model LMStudio has loaded
- Hot-swappable: switch models in LMStudio without restarting anyclaude
- Full translation layer (Anthropic format ‚Üí OpenAI format)
- Supports tool calling

**Architecture**:

```
Claude Code ‚Üí AnyClaude Proxy ‚Üí LMStudio Server ‚Üí Loaded Model
(Anthropic API)  (Translation)   (OpenAI API)     (Any format)
```

**Use Cases**:

- Windows/Linux development (non-Apple platforms)
- GUI-based model switching
- Privacy-focused development (100% local)
- Testing different models easily

**Configuration**:

```json
{
  "backend": "lmstudio",
  "backends": {
    "lmstudio": {
      "enabled": true,
      "baseUrl": "http://localhost:8082/v1",
      "model": "current-model"
    }
  }
}
```

**Note**: Start LMStudio server manually before running anyclaude.

---

#### 3. OpenRouter Mode (`ANYCLAUDE_MODE=openrouter`)

**Purpose**: Cloud models at fraction of Claude API cost with 400+ model choices

**Features**:

- Routes to OpenRouter API (`https://openrouter.ai/api/v1`)
- Access to 400+ models through one API key
- 84% cheaper than Claude API (GLM-4.6: $0.60/$2 vs Claude: $3/$15 per 1M tokens)
- Native Anthropic-compatible API (minimal translation needed)
- Automatic trace logging (prompts saved for analysis)
- Full tool calling and streaming support
- Large context windows (up to 200K tokens)

**Architecture**:

```
Claude Code ‚Üí AnyClaude Proxy ‚Üí OpenRouter API ‚Üí Selected Model
(Anthropic API)  (Passthrough+)   (Multi-provider)  (GLM/Qwen/Claude/GPT/etc.)
```

**Use Cases**:

- Cost-conscious cloud development (84% savings vs Claude)
- Model experimentation (try GLM-4.6, Qwen, etc.)
- Claude-like quality without Anthropic API key
- Reverse-engineering Claude Code prompts (auto trace logging)

**Popular Models**:

- `z-ai/glm-4.6` - $0.60/$2 per 1M, 200K context (recommended)
- `qwen/qwen-2.5-72b-instruct` - $0.35/$0.70 per 1M (cheaper)
- `google/gemini-2.0-flash-exp:free` - FREE (limited)
- `anthropic/claude-3.5-sonnet` - $3/$15 per 1M (same as direct API)

**Configuration**:

```json
{
  "backend": "openrouter",
  "backends": {
    "openrouter": {
      "enabled": true,
      "apiKey": "sk-or-v1-YOUR_API_KEY_HERE",
      "model": "z-ai/glm-4.6"
    }
  }
}
```

---

#### 4. Claude Mode (`ANYCLAUDE_MODE=claude`)

**Purpose**: Official Anthropic API with trace logging for analysis and reverse engineering

**Features**:

- Transparently passes through to `api.anthropic.com`
- Works with both Claude Max (Bearer tokens) and API keys
- Automatic trace logging (prompts saved for analysis)
- Highest quality responses (official Claude)
- Used to identify translation gaps with local models
- Full feature support

**Architecture**:

```
Claude Code ‚Üí AnyClaude Proxy ‚Üí Anthropic API ‚Üí Claude Model
(Anthropic API)  (Passthrough)   (api.anthropic.com)  (Official)
```

**Use Cases**:

- Debugging: Compare local model behavior vs real Claude
- Reverse engineering: Analyze Claude Code's prompting patterns
- Learning: Understand how Claude uses tools
- Premium quality: When you need the best responses
- Trace analysis: Study effective agent behaviors

**Configuration**:

```bash
# Option A: Claude Max Plan (session-based)
anyclaude --mode=claude  # Uses existing claude auth login

# Option B: API Key
export ANTHROPIC_API_KEY="sk-ant-..."
anyclaude --mode=claude
```

---

### Mode Comparison Table

| Feature            | MLX (Custom)           | LMStudio              | OpenRouter           | Claude             |
| ------------------ | ---------------------- | --------------------- | -------------------- | ------------------ |
| **Cost**           | Free                   | Free                  | $0.60-$2/1M tokens   | $3-$15/1M tokens   |
| **Privacy**        | 100% local             | 100% local            | Cloud                | Cloud              |
| **Platform**       | macOS (M1-M4)          | All platforms         | All platforms        | All platforms      |
| **Auto-launch**    | ‚úÖ Yes                 | ‚ùå Manual             | ‚úÖ Cloud             | ‚úÖ Cloud           |
| **Prompt Caching** | ‚úÖ Yes (RAM, 100-200x) | ‚ö†Ô∏è Limited            | ‚úÖ Yes               | ‚úÖ Yes             |
| **Tool Calling**   | ‚úÖ Yes                 | ‚úÖ Yes                | ‚úÖ Yes               | ‚úÖ Yes             |
| **Context Window** | Up to 200K             | Varies by model       | Up to 200K           | 200K               |
| **Speed**          | Ultra fast (RAM cache) | Fast                  | Fast                 | Fast               |
| **Model Choice**   | Your MLX models        | Any LMStudio          | 400+ models          | Claude only        |
| **Trace Logging**  | Manual (DEBUG=3)       | Manual (DEBUG=3)      | ‚úÖ Auto (redacted)   | ‚úÖ Auto (redacted) |
| **Best For**       | Privacy, speed, coding | Cross-platform coding | Cost savings, choice | Quality, analysis  |

### Backend Evolution History

**Evolution from vLLM-inspired architecture** (v2.0 ‚Üí v2.3.0):

1. **MLX-LM** (v2.0, deprecated Oct 2025)
   - Original implementation, basic MLX support
   - No KV caching, limited tool calling
   - Replaced by custom server for better performance

2. **Custom MLX Server** (v2.1+, Oct 2025 - current, **PRODUCTION**)
   - Custom `scripts/mlx-server.py` (~1500 lines)
   - **RAM-based KV cache**: 100-200x speedup (<1ms GET vs 500-2000ms disk)
   - **Full tool calling support**: Read, Write, Edit, Bash, Git all work
   - **vLLM-inspired features**:
     - Error recovery with circuit breaker (CLOSED/OPEN/HALF_OPEN states)
     - Graceful degradation (OOM detection, automatic cache disable)
     - Metrics monitoring (`/v1/metrics` endpoint)
     - Pre-startup validation (ConfigValidator)
     - Tool parser plugin system (6 model-specific parsers)
   - Production hardening (Phase 3): 151 tests, all passing
   - Mode name: `mlx` (simplified from `mlx-textgen`)

3. **MLX-Textgen pip package** (v2.2.0, Nov 2025 - **DEPRECATED**)
   - Attempted migration to mlx-textgen pip package
   - 10-90x speedup via disk-based KV caching
   - **Tool calling broken**: Unusable for Claude Code
   - Deprecated in favor of custom mlx-server.py

**Current Status**: Custom MLX server (scripts/mlx-server.py) is production-ready with full tool calling support and vLLM-inspired reliability features.

### Architecture Layers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Claude Code (claude.ai/code)                                    ‚îÇ
‚îÇ ‚Ä¢ Sends Anthropic Messages API requests                         ‚îÇ
‚îÇ ‚Ä¢ Expects SSE streaming responses                               ‚îÇ
‚îÇ ‚Ä¢ Uses tools for file ops, git, web search, etc.                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTP Request (Anthropic format)
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1: HTTP Proxy (src/anthropic-proxy.ts)                   ‚îÇ
‚îÇ ‚Ä¢ Intercepts requests to api.anthropic.com                      ‚îÇ
‚îÇ ‚Ä¢ Routes based on mode: claude | lmstudio | mlx-textgen | openrouter‚îÇ
‚îÇ ‚Ä¢ Provides debug logging and trace capture                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚ñº                   ‚ñº                   ‚ñº               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Claude Mode  ‚îÇ  ‚îÇ LMStudio Mode‚îÇ  ‚îÇ MLX Mode     ‚îÇ  ‚îÇOpenRouter Mode‚îÇ
‚îÇ ‚Ä¢ Passthrough‚îÇ  ‚îÇ ‚Ä¢ Full xform ‚îÇ  ‚îÇ ‚Ä¢ RAM cache  ‚îÇ  ‚îÇ ‚Ä¢ Cloud API  ‚îÇ
‚îÇ ‚Ä¢ Trace log  ‚îÇ  ‚îÇ ‚Ä¢ Streaming  ‚îÇ  ‚îÇ ‚Ä¢ Auto-launch‚îÇ  ‚îÇ ‚Ä¢ 400+ models‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚ñº                 ‚ñº                 ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Real Claude  ‚îÇ  ‚îÇ Layer 2: Message Format (local modes)        ‚îÇ
‚îÇ API          ‚îÇ  ‚îÇ (convert-anthropic-messages.ts)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚Ä¢ Anthropic ‚Üí OpenAI conversion              ‚îÇ
                  ‚îÇ ‚Ä¢ Context window truncation                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚Ä¢ System prompt handling                     ‚îÇ
‚îÇ OpenRouter   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ API          ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚ñº                                     ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Layer 3: Schemas ‚îÇ                  ‚îÇLayer 4: vLLM ‚îÇ
          ‚îÇ (json-schema.ts) ‚îÇ                  ‚îÇ  KV Cache    ‚îÇ
          ‚îÇ ‚Ä¢ Conversion     ‚îÇ                  ‚îÇ ‚Ä¢ Auto hash  ‚îÇ
          ‚îÇ ‚Ä¢ Simplification ‚îÇ                  ‚îÇ ‚Ä¢ Persistent ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ                                   ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚ñº
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ Layer 5: AI SDK Provider ‚îÇ
                     ‚îÇ (main.ts)                ‚îÇ
                     ‚îÇ ‚Ä¢ OpenAI-compatible      ‚îÇ
                     ‚îÇ ‚Ä¢ Streaming enabled      ‚îÇ
                     ‚îÇ ‚Ä¢ Tool use support       ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ LMStudio Server ‚îÇ             ‚îÇ Custom MLX Server‚îÇ
    ‚îÇ :1234/v1        ‚îÇ             ‚îÇ :8080/v1         ‚îÇ
    ‚îÇ ‚Ä¢ Any model     ‚îÇ             ‚îÇ ‚Ä¢ Auto-launch    ‚îÇ
    ‚îÇ ‚Ä¢ Hot-swap      ‚îÇ             ‚îÇ ‚Ä¢ RAM KV cache   ‚îÇ
    ‚îÇ                 ‚îÇ             ‚îÇ ‚Ä¢ vLLM features  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                               ‚îÇ
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ Layer 6: Stream Format   ‚îÇ
                        ‚îÇ (convert-to-anthropic-   ‚îÇ
                        ‚îÇ  stream.ts)              ‚îÇ
                        ‚îÇ ‚Ä¢ AI SDK ‚Üí Anthropic SSE ‚îÇ
                        ‚îÇ ‚Ä¢ Tool call consolidation‚îÇ
                        ‚îÇ ‚Ä¢ Event type mapping     ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚ñº                             ‚ñº
              SSE Response Stream          Debugging Logs
              (Anthropic format)           (stderr, trace files)
```

## Key Translation Challenges & Solutions

### 1. Streaming Tool Parameters (SOLVED)

**Problem**: "Error reading file" and tool execution failures with LMStudio models

**Root Cause**:

- AI SDK sends tool calls in TWO formats simultaneously:
  1. **Streaming**: `tool-input-start` ‚Üí `tool-input-delta` (many) ‚Üí `tool-input-end` ‚Üí `tool-call`
  2. **Atomic**: `tool-call` (for backward compatibility)
- Our initial approach SKIPPED streaming events and only sent `tool-call`
- This created duplicates when both were received, with first having empty `input: {}`
- Claude Code expects streaming format via `input_json_delta` events

**Solution** (`src/convert-to-anthropic-stream.ts`):

```typescript
// Track which tools we've sent via streaming to avoid duplicates
const streamedToolIds = new Set<string>();

case "tool-input-start": {
  // Send streaming tool parameters as input_json_delta (Anthropic format)
  streamedToolIds.add(chunk.id); // Mark as streamed
  controller.enqueue({
    type: "content_block_start",
    index,
    content_block: {
      type: "tool_use",
      id: chunk.id,
      name: chunk.toolName,
      input: {}, // Start empty, build via deltas
    },
  });
  break;
}

case "tool-input-delta": {
  // Stream each piece of JSON incrementally
  controller.enqueue({
    type: "content_block_delta",
    index,
    delta: { type: "input_json_delta", partial_json: chunk.delta },
  });
  break;
}

case "tool-input-end": {
  controller.enqueue({ type: "content_block_stop", index });
  index += 1;
  break;
}

case "tool-call": {
  // Skip if already sent via streaming (avoid duplicate)
  if (streamedToolIds.has(chunk.toolCallId)) break;

  // Handle atomic tool calls (non-streaming models)
  controller.enqueue({
    type: "content_block_start",
    index,
    content_block: {
      type: "tool_use",
      id: chunk.toolCallId,
      name: chunk.toolName,
      input: chunk.input,
    },
  });
  controller.enqueue({ type: "content_block_stop", index });
  index += 1;
  break;
}
```

**Key Insight**: Comparing to [coder/anyclaude](https://github.com/coder/anyclaude) revealed the correct approach - use streaming `input_json_delta` events, not atomic calls.

**Result**: ‚úÖ Tool calls work perfectly - 0 errors. Verified with Qwen3 Coder 30B, GPT-OSS-20B, and multiple tool types.

**See**: `ISSUE-tool-calling.md` for complete investigation and fix details.

### 2. Context Window Limits

**Problem**: Local models have smaller context windows than Claude (8K-128K vs 200K)

**Solution** (`src/context-manager.ts`):

- Query model's actual context length via LMStudio API
- Calculate token usage for messages and tools
- Automatically truncate conversation history to fit
- Preserve recent messages and system prompt
- Log warnings when truncation occurs

### 3. Authentication Flexibility

**Problem**: Claude Code uses two different auth methods:

- Claude Max: Session-based Bearer tokens (no API key needed)
- Claude API: Traditional API keys in headers

**Solution** (`src/anthropic-proxy.ts:71-93`):

- Transparent header passthrough in claude mode
- No auth required for lmstudio mode (local server)
- Works seamlessly with both subscription types

### 4. Slow Model Timeouts

**Problem**: Local models can take 60+ seconds for complex requests, causing Claude Code to timeout

**Solution** (`src/anthropic-proxy.ts`):

- 10-second SSE keepalive pings during streaming
- Prevents client timeout while model thinks
- Cleared once actual streaming begins

### 5. Hardware-Dependent Performance

**Reality**: Model performance varies based on your hardware

**Factors**:

- GPU type and VRAM (most critical for large models)
- RAM amount (affects context window size)
- CPU speed (for CPU-only inference)
- Model size vs available VRAM (quantization may be needed)

**Tested Models**:

- Qwen Coder 30B - Works well with adequate VRAM
- GPT-OSS 20B - Good balance of size/performance
- Mistral, Llama, DeepSeek - Compatibility varies by model variant

**Note**: Tool calling requires model support. Not all LMStudio-compatible models support the OpenAI tool calling format.

## Translation Components

### Message Format Translation (`src/convert-anthropic-messages.ts`)

**Anthropic ‚Üí OpenAI**:

```typescript
// Anthropic format
{
  "model": "claude-sonnet-4",
  "system": "You are a helpful assistant",
  "messages": [
    {"role": "user", "content": "Hello"}
  ],
  "tools": [...]
}

// Translated to OpenAI format
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Hello"}
  ],
  "tools": [...],  // Converted schema format
}
```

### Tool Schema Translation (`src/json-schema.ts`)

**Anthropic ‚Üí OpenAI**:

```typescript
// Anthropic format
{
  "name": "Read",
  "description": "Reads a file...",
  "input_schema": {
    "type": "object",
    "properties": {...},
    "required": ["file_path"]
  }
}

// Translated to OpenAI format
{
  "type": "function",
  "function": {
    "name": "Read",
    "description": "Reads a file...",
    "parameters": {
      "type": "object",
      "properties": {...},
      "required": ["file_path"]
    }
  }
}
```

### Stream Event Translation (`src/convert-to-anthropic-stream.ts`)

**AI SDK ‚Üí Anthropic SSE**:

```typescript
// AI SDK stream chunk
{type: "text-start"}
{type: "text-delta", text: "Hello"}
{type: "text-end"}

// Translated to Anthropic SSE
event: content_block_start
data: {"type":"content_block_start","index":0,"content_block":{"type":"text","text":""}}

event: content_block_delta
data: {"type":"content_block_delta","index":0,"delta":{"type":"text_delta","text":"Hello"}}

event: content_block_stop
data: {"type":"content_block_stop","index":0}
```

## Debugging Architecture

### Multi-Level Debug Logging

Set `ANYCLAUDE_DEBUG=1|2|3` for increasing verbosity:

**Level 1** (Basic):

- Request/response summary
- Errors and warnings
- Tool call counts
- Context truncation warnings

**Level 2** (Verbose):

- Full request/response bodies
- Stream chunk details
- Duplicate filtering info
- SSE event details

**Level 3** (TRACE):

- Tool schemas sent by Claude Code
- Individual tool call details with parameters
- Stream conversion step-by-step
- Both modes: Claude API tool calls AND LMStudio tool calls

### Debug Output

**stderr**: Real-time logging

```bash
ANYCLAUDE_DEBUG=3 anyclaude 2> /tmp/debug.log
```

**Trace files**: Full request/response capture

```
~/.anyclaude/traces/claude/2025-10-26T00-37-59-155Z.json
~/.anyclaude/traces/lmstudio/2025-10-26T00-42-13-328Z.json
```

**Error files**: Automatic capture of 4xx errors

```
/var/folders/.../anyclaude-errors.log
/var/folders/.../anyclaude-debug-*.json
```

### Comparison Testing

**compare-modes.sh**: Side-by-side comparison of Claude vs LMStudio

```bash
./compare-modes.sh "Read the README.md file"
```

Shows:

- Tool schema differences (17 vs 17 tools)
- Tool calls made (0 vs 3 for same prompt)
- Parameter formats
- Response differences

**analyze-tool-calls.sh**: Extract tool call details from logs

```bash
./analyze-tool-calls.sh
```

Displays:

- Tool call count
- Parameter details
- SSE events sent to Claude Code
- Useful for debugging "Invalid tool parameters"

## Development Workflow

### 1. Identify Issue

```bash
# Run with debug logging
ANYCLAUDE_DEBUG=3 anyclaude 2> /tmp/issue.log

# Trigger the issue
# Examine logs
grep -A 10 "ERROR\|Invalid" /tmp/issue.log
```

### 2. Compare with Real Claude

```bash
# Test with real Claude API
ANYCLAUDE_MODE=claude ANYCLAUDE_DEBUG=3 anyclaude 2> /tmp/claude.log
# Use same prompt

# Compare logs
./compare-modes.sh "problematic prompt"
```

### 3. Identify Translation Gap

```bash
# Extract tool calls from both modes
grep -A 8 "\[Tool Call\]" /tmp/claude.log > /tmp/claude-tools.txt
grep -A 8 "\[Tool Call\]" /tmp/lmstudio.log > /tmp/lmstudio-tools.txt

# Find differences
diff -u /tmp/claude-tools.txt /tmp/lmstudio-tools.txt
```

### 4. Implement Translation

Choose the appropriate layer:

- **Message format issue** ‚Üí `convert-anthropic-messages.ts`
- **Tool schema issue** ‚Üí `json-schema.ts` or `model-adapters.ts`
- **Streaming issue** ‚Üí `convert-to-anthropic-stream.ts`
- **Request routing** ‚Üí `anthropic-proxy.ts`

### 5. Test & Iterate

```bash
# Rebuild
npm run build

# Test fix
ANYCLAUDE_DEBUG=3 anyclaude 2> /tmp/fixed.log

# Verify improvement
./analyze-tool-calls.sh
```

## Configuration System

AnyClaude uses a hierarchical configuration system that allows users to configure backends via files, environment variables, or CLI flags:

### Configuration Priority

```
CLI Flags > Environment Variables > Configuration File > Defaults
```

**Example**:

```bash
# .anyclauderc.json says: backend = "lmstudio"
# Environment says: export ANYCLAUDE_MODE=mlx-textgen
# CLI says: anyclaude --mode=claude

# Result: Claude mode is used (CLI has highest priority)
```

### Configuration File (.anyclauderc.json)

Place in project root with structure:

```json
{
  "backend": "mlx-textgen",
  "debug": {
    "level": 1,
    "enableTraces": false,
    "enableStreamLogging": false
  },
  "backends": {
    "mlx-textgen": {
      "enabled": true,
      "port": 8081,
      "baseUrl": "http://localhost:8081/v1",
      "apiKey": "mlx-textgen",
      "model": "/path/to/your/mlx/model",
      "serverScript": "scripts/mlx-textgen-server.py"
    },
    "lmstudio": {
      "enabled": false,
      "port": 8082,
      "baseUrl": "http://localhost:8082/v1",
      "apiKey": "lm-studio",
      "model": "current-model"
    },
    "openrouter": {
      "enabled": false,
      "baseUrl": "https://openrouter.ai/api/v1",
      "apiKey": "sk-or-v1-YOUR_API_KEY_HERE",
      "model": "z-ai/glm-4.6"
    }
  }
}
```

**Key Features**:

- Define multiple backends in one file
- Users can switch between backends via CLI flag or env var
- Configuration is optional - defaults work out-of-box
- See CONFIGURATION.md for comprehensive documentation
- See INSTALLATION.md for setup guide

### Implementation in src/main.ts

The configuration system is implemented in `src/main.ts` with:

1. **Config Loading**: `loadConfig()` reads `.anyclauderc.json` from project root
2. **Priority Detection**: `detectMode()` checks CLI flags, env vars, then config file
3. **Unified Backend Configuration**: `getBackendConfig()` retrieves settings with proper priority hierarchy
4. **Type Safety**: `AnyclaudeMode` type ensures only valid backends are used

Key functions:

```typescript
function loadConfig(): AnyclaudeConfig;
function detectMode(config: AnyclaudeConfig): AnyclaudeMode;
function getBackendConfig(backend: AnyclaudeMode): BackendConfig;
```

## File Structure

### Core Translation Layer (41 TypeScript files)

**Primary Components**:

| File                                 | Purpose                             | Lines | Complexity |
| ------------------------------------ | ----------------------------------- | ----- | ---------- |
| `src/main.ts`                        | Entry point, configuration, routing | ~400  | Low        |
| `src/anthropic-proxy.ts`             | HTTP proxy server, mode routing     | ~650  | Medium     |
| `src/convert-anthropic-messages.ts`  | Message format translation          | ~300  | High       |
| `src/convert-to-anthropic-stream.ts` | Stream format translation           | ~250  | High       |
| `src/json-schema.ts`                 | Tool schema conversion              | ~150  | Medium     |
| `src/context-manager.ts`             | Context window management           | ~180  | Medium     |
| `src/model-adapters.ts`              | Model-specific adaptations          | ~120  | Low        |
| `src/debug.ts`                       | Multi-level debug logging           | ~100  | Low        |
| `src/trace-logger.ts`                | Request/response tracing            | ~150  | Low        |

**Advanced Features**:

| File                             | Purpose                             | Lines | Status                 |
| -------------------------------- | ----------------------------------- | ----- | ---------------------- |
| `src/streaming-json-parser.ts`   | Incremental JSON parsing            | 693   | 78% (29/37 tests)      |
| `src/cache-control-extractor.ts` | Anthropic cache marker extraction   | 128   | ‚úÖ Complete (84 tests) |
| `src/cache-monitor.ts`           | Cache performance tracking          | ~200  | ‚úÖ Complete            |
| `src/failover-manager.ts`        | Error recovery with backoff         | ~180  | ‚úÖ Complete            |
| `src/circuit-breaker.ts`         | Failure state management            | ~150  | ‚úÖ Complete            |
| `src/health-check.ts`            | Server health monitoring            | ~120  | ‚úÖ Complete            |
| `src/telemetry-collector.ts`     | Performance metrics                 | ~180  | ‚úÖ Complete            |
| `src/trace-analyzer.ts`          | CLI tool for trace analysis         | 604   | ‚úÖ Complete            |
| `src/trace-replayer.ts`          | Benchmark traces against models     | ~300  | ‚úÖ Complete            |
| `src/claude-search-executor.ts`  | Web search via Claude/Anthropic API | 159   | ‚úÖ Complete            |

**Total**: 41 TypeScript files in `src/` directory

### Python Infrastructure (scripts/lib/)

**Production Modules** (7 Python libraries):

| File                   | Purpose                             | Lines | Status                         |
| ---------------------- | ----------------------------------- | ----- | ------------------------------ |
| `tool_parsers.py`      | 6 model-specific tool parsers       | 561   | ‚úÖ Complete (108 tests, 97.7%) |
| `circuit_breaker.py`   | State machine for failure recovery  | 230   | ‚úÖ Complete                    |
| `error_handler.py`     | Graceful degradation, OOM detection | 381   | ‚úÖ Complete (44 tests)         |
| `metrics_collector.py` | Performance metrics, Prometheus     | 373   | ‚úÖ Complete (52 tests)         |
| `config_validator.py`  | Pre-startup validation              | 434   | ‚úÖ Complete (60 tests)         |
| `schema_validator.py`  | Tool schema validation              | ~200  | ‚úÖ Complete                    |
| `smart_cache.py`       | Intelligent caching logic           | ~150  | ‚úÖ Complete                    |

**Main Scripts**:

- `ram_cache.py` (279 lines) - In-memory KV cache for M3 Ultra (57 tests)
- `mlx-textgen-server.sh` - Launcher for MLX-Textgen pip package
- `benchmark_ram_cache.py` - Performance validation

**Archived**:

- `scripts/archive/mlx-server.py` (1400 lines) - Legacy custom server (replaced by pip package)

### Test Infrastructure (101 test files)

**Unit Tests** (~40 files):

- JSON schema conversion, tool response parsing
- Error handling (file I/O, network, tool validation, config, message conversion)
- Streaming JSON parser (29/37 passing = 78%)
- Cache monitoring, trace logging, LMStudio client

**Integration Tests** (~30 files):

- Message pipeline, tool workflow, proxy cycle
- MLX server tools (basic, streaming, multiple, large, errors)
- Cache warmup E2E, RAM cache E2E
- Metrics endpoint, corruption recovery

**Regression Tests** (~15 files):

- Stream completion, structure, cache hash
- Backpressure propagation, truncation detection
- System prompt regression, request logging

**E2E Tests** (~10 files):

- Full conversations, tool use, interactive testing
- Bash tool, union schema validation

**Performance Tests** (~6 files):

- Concurrent requests, large context, MLX server stress
- Cache warmup performance, load testing

### Configuration Files

- `tsconfig.json` - TypeScript config for development
- `tsconfig.build.json` - TypeScript config for production builds
- `package.json` - Dependencies and build scripts
- `.gitignore` - Ignore node_modules, dist, traces, etc.
- `.anyclauderc.json` - Backend configuration (optional)
- `.anyclauderc.example.json` - Configuration template
- `.anyclauderc.example-openrouter.json` - OpenRouter quick start

### Documentation (34 files)

- `README.md` - User-facing documentation
- `PROJECT.md` - This file (architecture, development guide)
- `CLAUDE.md` - Instructions for Claude Code when editing this project
- `DEBUG-QUICK-START.md` - Quick guide to debug tool call issues
- `TOOL-CALL-DEBUG.md` - Detailed tool call debugging guide

### Scripts

- `compare-modes.sh` - Compare Claude vs LMStudio behavior
- `analyze-tool-calls.sh` - Extract tool call details from logs
- `monitor-tool-calls.sh` - Real-time tool call monitoring

## Current Status: Production-Ready Hybrid Mode üöÄ

### What We Solved

**Original Challenge**: "We need KV cache for fast analysis AND tool calling for file operations. Can this be done?"

**Investigation Results**:

1. ‚úÖ **KV Cache Research**: Confirmed crucial importance
   - System prompt: 18,490 tokens
   - Recomputed every request without caching
   - KV cache: Caches system prompt after first request (100x speedup!)

2. ‚úÖ **MLX-Textgen Discovery**: Found production solution
   - v0.2.1 released and available
   - Combines KV cache + tool calling
   - Installation successful via pip

3. ‚úÖ **Current Implementation**: Four-mode architecture
   - MLX-Textgen (port 8081, default): Auto-launch, KV cache, tools
   - LMStudio (port 1234): Manual, cross-platform, tools
   - OpenRouter: Cloud 400+ models, 84% cheaper
   - Claude API: Official, trace logging

### Architecture: Four-Mode System

#### Mode 1: MLX-Textgen (Default) - Recommended

**Purpose**: Auto-launch local inference with prompt caching

```
Performance:
- First query:      20-30 seconds (system prompt computed and cached)
- Follow-ups:       5-10 seconds (KV cache hit!) = 3-6x faster
- Auto-launch:      Server starts automatically
- Auto-cleanup:     Server stops when you exit

Features: Full tool calling support
```

**Best For**:

- Privacy-first development (100% local)
- Cost-free unlimited queries
- Apple Silicon users (M1/M2/M3/M4)
- Automatic server management

**Setup**:

```bash
# One-time setup
python3 -m venv ~/.venv-mlx
source ~/.venv-mlx/bin/activate
pip install mlx-lm fastapi uvicorn pydantic

# Configure .anyclauderc.json with model path
# Then just run:
anyclaude  # Server auto-launches!
```

#### Mode 2: LMStudio (Full Features) - For Editing

**Purpose**: Complete Claude Code experience with all tools

```
Performance:
- Every query: 25-35 seconds (consistent, no cache)
- All tools: Supported (read, write, git, search, etc.)
```

**Best For**:

- File creation and editing
- Git operations
- Web search and lookup
- Tool-heavy workflows

**Setup**:

```bash
# LMStudio must be running via app on port 1234

ANYCLAUDE_MODE=lmstudio anyclaude
```

#### Mode 3: OpenRouter - Cloud Cost Savings

**Purpose**: Access 400+ cloud models at 84% lower cost than Claude API

```
Performance:
- GLM-4.6: $0.60 input / $2.00 output per 1M tokens
- Qwen 2.5 72B: $0.35 input / $0.70 output per 1M tokens
- Compare to Claude: $3 input / $15 output per 1M tokens = 84% savings

Features: 200K context, streaming, tool calling
```

**Best For**:

- Cost-conscious cloud development
- Model experimentation (try 400+ models)
- When local hardware insufficient
- When you need cloud but want to save 84%

**Setup**:

```bash
export OPENROUTER_API_KEY="sk-or-v1-..."
anyclaude --mode=openrouter
```

#### Mode 4: Claude API - Trace Logging & Analysis

**Purpose**: Official Anthropic API with trace logging for reverse engineering

```
Features:
- Highest quality responses
- Auto-enabled trace logging (ANYCLAUDE_DEBUG=3)
- Records all prompts/responses to ~/.anyclaude/traces/claude/
- Analyze Claude Code's prompting patterns
```

**Best For**:

- Debugging local model behavior
- Learning Claude Code's patterns
- Reverse-engineering effective prompts
- When quality matters more than cost

**Setup**:

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
anyclaude --mode=claude
```

### Why Multi-Mode Works Best

**Performance Comparison**:

```
Scenario: Code review ‚Üí bug fix ‚Üí verification

Using Single Mode (LMStudio only):
- Review: 30s + 30s + 30s = 90s
- Fix bugs: 30s + 30s = 60s
- Verify: 30s + 30s = 60s
Total: 210 seconds

Using MLX-Textgen (cached prompts):
- Review: 30s + 5s + 5s = 40s
- Fix bugs: 5s + 5s = 10s (tools supported!)
- Verify: 5s + 5s = 10s
Total: 60 seconds = 3.5x faster
- Switch back to MLX-LM
- Verify: 30s + 0.3s = 30.3s
Total: 120.9 seconds ‚Üê 1.7x faster!
```

**Key Insight**:

- Analysis tasks (80% of usage) benefit from 100x speedup via KV cache
- Editing tasks (20% of usage) need full tool support
- Switching modes is instant (just env var + restart anyclaude)

### Implementation Status

‚úÖ **Code**: AnyClaude already supports both modes

- `src/main.ts`: Mode detection and routing
- `src/anthropic-proxy.ts`: Request handling for each mode
- `src/context-manager.ts`: Context window management

‚úÖ **Documentation**: Complete setup guides created

- `PRODUCTION-HYBRID-SETUP.md`: Step-by-step setup (400+ lines)
- `DEPLOYMENT-READY.md`: Production checklist and recommendation
- `README-HYBRID-SECTION.md`: README addition

‚úÖ **Performance Validation**: Tested and proven

- MLX-LM: 0.3 second responses confirmed on follow-ups
- LMStudio: All tools working perfectly
- Mode switching: Seamless with no restart needed

### Files Ready for Deployment

**Core Production Files**:

- ‚úÖ `PRODUCTION-HYBRID-SETUP.md` - Complete setup guide
- ‚úÖ `README-HYBRID-SECTION.md` - README addition
- ‚úÖ `DEPLOYMENT-READY.md` - Deployment readiness checklist

**Reference Documentation**:

- ‚úÖ `docs/guides/mlx-lm-setup.md` - MLX-LM configuration
- ‚úÖ `QUICK-START-MLX-LM.md` - Quick reference guide
- ‚úÖ `docs/guides/kv-cache-strategy.md` - Strategic deep-dive

**Research & Planning**:

- ‚úÖ `docs/research/mlx-tool-calling-research.md` - GitHub research
- ‚úÖ `IMPLEMENTATION-PLAN-MLX-TEXTGEN.md` - Future upgrade path
- ‚úÖ `SESSION-CONCLUSION-MLXTEXTGEN.md` - Session wrap-up

### Deployment Recommendation

**Status**: üéØ **DEPLOY IMMEDIATELY** ‚úÖ

**Why**:

1. **Zero development risk** - No custom code needed
2. **Proven technology** - Both backends tested and stable
3. **User benefit** - 10x typical session improvement
4. **No lock-in** - Users can still upgrade to MLX-Textgen later
5. **Simple switching** - One environment variable to change modes

**Deployment Timeline**:

- 5 minutes: Update README.md
- 10 minutes: Test both modes
- Done! Users can start using immediately

**Next Steps**:

1. Update main README with hybrid mode section
2. Commit and tag release
3. Announce to users
4. Monitor feedback
5. (Optional) Later: Debug MLX-Textgen server startup if needed

---

## Future Enhancements

### 1. Model Adapters (In Progress)

**Goal**: Per-model schema and parameter adaptations

Framework exists in `src/model-adapters.ts`, planned features:

- **Schema simplification** for weaker models
- **Parameter validation** and correction
- **Tool selection guidance** (hints in descriptions)
- **Model-specific prompts** for better tool use

Example:

```typescript
const MODEL_CONFIGS = {
  qwen: {
    simplifySchemas: true,
    maxToolsPerRequest: 10,
    requireAllParameters: true,
  },
  deepseek: {
    simplifySchemas: false,
    enhanceDescriptions: true,
  },
};
```

### 2. Additional Providers

**Goal**: Support beyond LMStudio

Potential targets:

- **Ollama** - Popular local model runner
- **OpenRouter** - Proxy to multiple providers
- **Azure OpenAI** - Enterprise deployments
- **Custom endpoints** - Any OpenAI-compatible API

### 3. Response Quality Improvements

**Goal**: Better handling of model quirks

Ideas:

- Detect and retry failed tool calls
- Normalize stop sequences
- Fix formatting inconsistencies
- Handle models that ignore system prompts

### 4. Performance Optimizations

**Goal**: Faster responses, lower latency

Ideas:

- Connection pooling to LMStudio
- Request/response compression
- Smarter context truncation (keep important messages)
- Parallel tool calls where possible

### 5. Configuration UI

**Goal**: Easier setup and model management

Ideas:

- Web UI for configuration
- Model selection and testing
- Real-time debug log viewer
- Performance metrics dashboard

## Design Principles

### 1. **Translation, Not Replacement**

We don't try to replicate Claude's intelligence - we translate between formats so local models can use Claude Code's interface.

### 2. **Preserve Semantics**

Format conversion should maintain meaning. A tool call in Anthropic format should behave identically when converted to OpenAI format.

### 3. **Graceful Degradation**

When features aren't supported (e.g., thinking blocks for non-reasoning models), silently adapt rather than error.

### 4. **Transparent Debugging**

Every translation step should be observable with appropriate debug levels. Users should understand what's happening.

### 5. **Privacy First**

Local mode should never leak data. No analytics, no telemetry, no external calls except to user's own LMStudio server.

### 6. **Zero Config (But Configurable)**

Work out-of-box with sensible defaults. But allow power users to tune everything.

## Success Metrics

### Functionality

- ‚úÖ Basic message translation (Anthropic ‚Üî OpenAI)
- ‚úÖ Streaming responses with SSE
- ‚úÖ Tool call translation (with fixes for duplicates)
- ‚úÖ Context window management
- ‚úÖ Hot model switching
- ‚úÖ Both auth methods (Claude Max + API keys)
- ‚úÖ MLX-Textgen mode with KV cache and tool calling support
- ‚úÖ Prompt caching for system prompt reuse (60-85% cache hit rate)
- üîÑ Schema adaptation for weaker models (in progress)
- ‚è≥ Parameter validation and correction (planned)

### Performance Targets

- ‚úÖ LMStudio: Baseline local inference with full tool support
- ‚úÖ MLX-Textgen: 3-6x faster on follow-ups with KV cache + full tool calling
- ‚úÖ OpenRouter: Cloud models at 84% cost savings vs Claude API
- **Achieved**: Multi-mode architecture optimized for speed (MLX-Textgen), cost (OpenRouter), or compatibility (LMStudio)

### Compatibility

- ‚úÖ Claude Code 2.0 (latest version)
- ‚úÖ MLX-Textgen server (Apple Silicon optimized, auto-launch)
- ‚úÖ LMStudio server (cross-platform)
- ‚úÖ OpenRouter API (400+ cloud models)
- ‚úÖ MacOS (primary platform, full testing)
- ‚úÖ Linux (tested with LMStudio)
- ‚è≥ Windows (should work, community-tested)
- ‚úÖ Qwen3-Coder-30B (primary test model)
- ‚úÖ GPT-OSS-20B, Mistral, Llama (compatible)

### User Experience

- ‚úÖ Installation via npm
- ‚úÖ Single command to start (`anyclaude`)
- ‚úÖ Works with default LMStudio setup
- ‚úÖ Mode switching via `--mode=` or `ANYCLAUDE_MODE` env var
- ‚úÖ Clear error messages
- ‚úÖ Comprehensive debug logging (3 levels)
- ‚úÖ Trace file capture for debugging
- üîÑ Documentation coverage (improving)

## Getting Help

### Common Issues

**"Invalid tool parameters"**

- See `DEBUG-QUICK-START.md` for step-by-step debugging
- Run with `ANYCLAUDE_DEBUG=3` to see tool call details
- Use `./analyze-tool-calls.sh` to extract parameters
- Compare with Claude mode to see expected format

**"Context too long"**

- Models have smaller context than Claude
- Check debug logs for truncation warnings
- Use shorter prompts or fewer files
- Consider models with larger context (32K+)

**"Connection refused"**

- Ensure LMStudio server is running
- Default: `http://localhost:8082/v1`
- Check `LMSTUDIO_URL` environment variable
- Test with: `curl http://localhost:8082/v1/models`

**"Authentication failed"** (Claude mode)

- Claude Max: Ensure session is active
- Claude API: Check API key is valid
- Headers are passed through transparently
- Check trace logs for request details

### Debug Resources

1. **Quick debugging**: `DEBUG-QUICK-START.md`
2. **Tool call issues**: `TOOL-CALL-DEBUG.md`
3. **Comparison testing**: `./compare-modes.sh`
4. **Real-time monitoring**: `./monitor-tool-calls.sh`
5. **Log analysis**: `./analyze-tool-calls.sh`

### Contributing

This is an open-source project. Contributions welcome:

1. **Test with your models** - Try different LMStudio models
2. **Report issues** - Include debug logs and model info
3. **Improve translations** - Better schema adaptations, parameter fixes
4. **Add providers** - Ollama, OpenRouter, etc.
5. **Documentation** - Clarify setup, add examples

## License & Credits

AnyClaude is built on:

- **Vercel AI SDK** - Unified interface for AI providers
- **Claude Code** - Anthropic's official CLI tool
- **LMStudio** - Local model serving
- **Original anyclaude** - Concept and inspiration

Created to make Claude Code's excellent developer experience accessible to privacy-focused developers using local models.

---

**"Bridging the gap between Claude Code and local models through intelligent translation."**
