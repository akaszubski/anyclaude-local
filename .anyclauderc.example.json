{
  "// COMMENT": "AnyClaude Configuration File - LMStudio, OpenRouter, and MLX Cluster",
  "// USAGE": "Copy this to .anyclauderc.json and customize for your setup",
  "// DOCS": "See README.md for detailed configuration guide",

  "backend": "lmstudio",
  "debug": {
    "level": 0,
    "enableTraces": false,
    "enableStreamLogging": false
  },
  "webSearch": {
    "localSearxngUrl": "http://localhost:8080",
    "preferLocal": true,
    "enableFallback": true
  },
  "backends": {
    "lmstudio": {
      "enabled": true,
      "baseUrl": "http://localhost:8081/v1",
      "apiKey": "lm-studio",
      "model": "current-model",
      "modelPath": "/path/to/mlx/model",
      "compatibility": "legacy",
      "description": "Local MLX Worker - auto-starts when using localhost",
      "autoStartServer": true,
      "startupTimeout": 120000,
      "truncateSystemPrompt": false,
      "systemPromptMaxTokens": 2000,
      "smartSystemPrompt": false,
      "smartPromptMode": "simple",
      "safeSystemFilter": true,
      "filterTier": "auto",
      "distributed": {
        "enabled": false,
        "peers": [{"host": "10.55.0.2", "memory_gb": 512}],
        "strategy": "pipeline",
        "connection": "auto",
        "local_memory_gb": 128
      }
    },
    "openrouter": {
      "enabled": false,
      "baseUrl": "https://openrouter.ai/api/v1",
      "apiKey": "sk-or-v1-YOUR_API_KEY_HERE",
      "model": "google/gemini-2.5-flash",
      "description": "OpenRouter - 400+ cloud models, 88% cheaper than Claude API"
    },
    "claude": {
      "enabled": false,
      "description": "Real Anthropic Claude API (requires ANTHROPIC_API_KEY env var)"
    },
    "mlx-cluster": {
      "enabled": false,
      "description": "Distributed MLX inference across multiple Apple Silicon Macs",
      "discovery": {
        "mode": "static",
        "nodes": [
          {
            "id": "mac-studio-1",
            "url": "http://192.168.1.100:8081"
          },
          {
            "id": "macbook-pro-1",
            "url": "http://192.168.1.101:8081"
          }
        ]
      },
      "health": {
        "checkIntervalMs": 30000,
        "timeoutMs": 5000,
        "maxConsecutiveFailures": 3,
        "unhealthyThreshold": 0.5
      },
      "routing": {
        "strategy": "cache-aware",
        "maxRetries": 2,
        "retryDelayMs": 1000
      },
      "cache": {
        "maxCacheAgeSec": 600,
        "minCacheHitRate": 0.7,
        "maxCacheSizeTokens": 100000
      }
    }
  },

  "// BACKEND OPTIONS": {
    "lmstudio": "Local models - cross-platform, manual server management",
    "openrouter": "Cloud models - access Gemini, Qwen, Claude, GPT-4 via one API (recommended for best quality)",
    "claude": "Official Claude API - full features, highest cost",
    "mlx-cluster": "Distributed inference - multiple Macs with shared KV cache (for MLX servers)"
  },

  "// MLX CLUSTER CONFIGURATION (Issue #22-32)": {
    "discovery.mode": "static | dns | kubernetes - how to find cluster nodes",
    "discovery.nodes": "Array of {id, url} for static node configuration",
    "health.checkIntervalMs": "How often to check node health (default: 30000ms = 30s)",
    "health.timeoutMs": "Health check timeout (default: 5000ms = 5s)",
    "health.maxConsecutiveFailures": "Failures before marking unhealthy (default: 3)",
    "health.unhealthyThreshold": "Failure rate threshold (0.0-1.0, default: 0.5 = 50%)",
    "routing.strategy": "round-robin | least-loaded | cache-aware | latency-based",
    "routing.maxRetries": "Retry attempts on node failure (default: 2)",
    "routing.retryDelayMs": "Delay between retries (default: 1000ms = 1s)",
    "cache.maxCacheAgeSec": "Max cache entry age before invalidation (default: 600s = 10min)",
    "cache.minCacheHitRate": "Min hit rate for cache optimization (0.0-1.0, default: 0.7)",
    "cache.maxCacheSizeTokens": "Max KV cache size in tokens (default: 100000)"
  },

  "// ROUTING STRATEGIES": {
    "round-robin": "Simple rotation through healthy nodes",
    "least-loaded": "Route to node with fewest active requests",
    "cache-aware": "Prefer nodes with matching system prompt cache (RECOMMENDED)",
    "latency-based": "Route to node with lowest average response time"
  },

  "// WEB SEARCH CONFIGURATION (Issue #49)": {
    "localSearxngUrl": "URL of your self-hosted SearxNG instance (default: http://localhost:8080)",
    "preferLocal": "Try local SearxNG first before cloud APIs (privacy-first)",
    "enableFallback": "Fall back to cloud search if local fails (Anthropic/Tavily/Brave)",
    "SEARXNG_URL": "Env var to enable local search (set to your SearxNG URL)",
    "Quick_Start": "Run: scripts/docker/start-searxng.sh to launch local SearxNG",
    "Benefits": "Privacy, no rate limits, no API costs, full control"
  },

  "// OPTIMIZATION STRATEGIES (Issue #21)": {
    "safeSystemFilter": "Enable intelligent prompt optimization that preserves tool calling (default: true for LMStudio, false for others)",
    "filterTier_auto": "Automatically select tier based on prompt size (recommended)",
    "filterTier_minimal": "Light optimization - remove optional sections only",
    "filterTier_moderate": "Balanced - remove verbose explanations",
    "filterTier_aggressive": "Heavy - remove non-critical guidelines",
    "filterTier_extreme": "Maximum reduction while preserving core identity and tools",
    "smartSystemPrompt": "Context-aware dynamic optimization (experimental, higher risk)",
    "truncateSystemPrompt": "Simple size-based truncation (used as fallback)",
    "NOTE": "Safe filter takes priority over truncation. Smart prompt takes priority over safe filter."
  },

  "// POPULAR OPENROUTER MODELS": {
    "google/gemini-2.5-flash": "$0.30/$2.50 per 1M tokens - 1M context, thinking mode (DEFAULT, best value)",
    "google/gemini-3-pro-preview": "$2/$12 per 1M tokens - 1M context, state-of-the-art reasoning",
    "z-ai/glm-4.6": "$0.60/$2 per 1M tokens - 200K context, excellent for coding",
    "qwen/qwen-2.5-72b-instruct": "$0.35/$0.70 per 1M tokens - cheapest option",
    "google/gemini-2.0-flash-001": "$0.10/$0.40 per 1M tokens - fastest/cheapest",
    "anthropic/claude-3.5-sonnet": "$3/$15 per 1M tokens - highest quality",
    "openai/gpt-4": "$10/$30 per 1M tokens - via OpenRouter"
  },

  "// AUTO-START SERVER (Issue #42)": {
    "autoStartServer": "Auto-start MLX Worker when using localhost (default: true)",
    "modelPath": "Path to MLX model directory (required for auto-start)",
    "startupTimeout": "Server startup timeout in ms (default: 120000 = 2min)",
    "MLX_MODEL_PATH": "Env var alternative to modelPath config",
    "NOTE": "Server only auto-stops if anyclaude started it (existing servers are preserved)"
  },

  "// ENVIRONMENT VARIABLES": {
    "ANYCLAUDE_MODE": "Override backend (claude|lmstudio|openrouter|mlx-cluster)",
    "ANYCLAUDE_DEBUG": "Debug level (0=off, 1=basic, 2=verbose, 3=trace with prompts)",
    "OPENROUTER_API_KEY": "Your OpenRouter API key (get from openrouter.ai)",
    "OPENROUTER_MODEL": "Override OpenRouter model",
    "ANTHROPIC_API_KEY": "Your Anthropic API key (for claude mode)",
    "LMSTUDIO_MODEL": "Override LMStudio model",
    "MLX_MODEL_PATH": "Path to MLX model for auto-start"
  },

  "// QUICK START": {
    "LMStudio": "1. Load a model in LMStudio, 2. Set backend: 'lmstudio', 3. Run: anyclaude",
    "OpenRouter": "1. Get API key from openrouter.ai, 2. Set backend: 'openrouter', 3. Add apiKey, 4. Run: anyclaude",
    "MLX Cluster": "1. Start MLX workers on each Mac, 2. Set backend: 'mlx-cluster', 3. Configure nodes, 4. Run: anyclaude --mode=mlx-cluster"
  },

  "// COST COMPARISON (per 1M tokens)": {
    "Gemini 2.5 Flash (DEFAULT)": "$0.30 input / $2.50 output - 88% cheaper than Claude, thinking mode",
    "Gemini 2.0 Flash": "$0.10 input / $0.40 output - cheapest cloud option",
    "Qwen 2.5 72B": "$0.35 input / $0.70 output - best value for basic coding",
    "GLM-4.6": "$0.60 input / $2.00 output - good for coding",
    "Claude 3.5 Sonnet": "$3.00 input / $15.00 output - highest quality",
    "Local models (LMStudio)": "$0 (hardware costs only)",
    "MLX Cluster": "$0 (hardware costs only, distributed across multiple Macs)"
  }
}
